---
title: "Introduction to frbayes"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{introduction_to_frbayes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(frbayes)
library(ggplot2)
library(purrr)
library(dplyr)
library(tidyr)
```


In this vignette, we provide an introduction to `frbayes`, a package for fitting to functional response data.

## Synthetic study
### Stochastic models
We first show how we can fit a functional response model to synthetically generated data, where the parameters of the process are known. We assume that data generating process is a stochastic Rogers-II-type model, where the process is assumed to follow a chemical reaction equation of the form:

$$
\text{prey} \xrightarrow{\text{rate}} \text{prey} - 1,
$$
where $\text{prey}$ means the number of prey remaining at a point in time, and the rate of this reaction is given by:

$$
\text{rate} = \frac{a \cdot \text{prey}}{1 + a \cdot h \cdot \text{prey}},
$$

where $a$ is a capture rate and $h$ is a handling time. Here, we assume the true data generating process is represented by the above model with $a=2$ and $h=0.1$.

We can simulate one such experiment, where we suppose the initial number of prey is 10 and the experiment takes place over a maximum of 5 time units. Here, we show how the number of prey eaten (10 - the number of prey remaining) increases with time. When the number of prey eaten reaches 10, the initial number of prey, there are no more prey to eat and the experiment ends.
```{r}
# rogersII parameters
true_parameters <- list(a=2, h=0.1)

# stochastic simulation
df_single_replicate <- simulate_trajectory(
  n_prey_initial = 10,
  time_max = 5,
  model = model_rogersII(),
  parameters = true_parameters
)

# plot
df_single_replicate %>% 
  ggplot(aes(x=time, y=n_prey_eaten)) +
  geom_step() +
  scale_y_continuous(n.breaks = 10) +
  scale_x_continuous(limits = c(0, 5))
```
Our simulation method is _stochastic_, meaning we get a variety of such trajectories each time the experiment is run. We show now 10 such trajectories.
```{r}
# generate many trajectories
n_trajectories <- 10
for(i in 1:n_trajectories) {
  
  df_single_replicate <- simulate_trajectory(
    n_prey_initial = 10,
    time_max = 10,
    model = model_rogersII(),
    parameters = true_parameters
  ) %>% 
    dplyr::mutate(trajectory_id=i)
  
  if(i == 1)
    df_all_trajectories <- df_single_replicate
  else
    df_all_trajectories <- df_all_trajectories %>% 
      dplyr::bind_rows(df_single_replicate)
}

# plot
df_all_trajectories %>% 
  ggplot(aes(x=time,
             y=n_prey_eaten,
             group=as.factor(trajectory_id))) +
  geom_step(alpha=0.5) +
  scale_y_continuous(n.breaks = 10)
```
### Typical experiment setup
In a typical experiment, there is a time chosen at which to observe the number of prey eaten. Here, we assume this is at $\text{time}=1$. This effectively means that we slice through the trajectories at this time and record the number of prey at that point: here, this gives us a number of prey eaten that roughly ranges around the 4-9 range.
```{r}
# plot slice
df_all_trajectories %>% 
  ggplot(aes(x=time,
             y=n_prey_eaten,
             group=as.factor(trajectory_id))) +
  geom_step(alpha=0.5) +
  scale_y_continuous(n.breaks = 10) +
  geom_vline(xintercept = 1, linetype=2, colour="orange")
```
Usually, experiments then repeat this exercise but under a variety of experimental conditions, particularly across a range of initial prey counts.

We now replicate one such experiment. We suppose that 100 replicates were performed at initial prey counts of: 5, 10, 20, 30, 40, and that the system was observed at $time=1$. Using this experimental setup, we can simulate the output of running one such experiment.
```{r}
# experiment details
experimental_setup <- data.frame(
  n_prey_initial = c(5, 10, 20, 30, 40),
  n_replicates = 100
)

# generate synthetic data
df <- simulate_study(
  data=experimental_setup,
  time_max = 1,
  model = model_rogersII(),
  parameters = true_parameters
)

# plot data
df %>% 
  ggplot(aes(x=n_prey_initial, y=n_prey_eaten)) +
  geom_jitter(height = 0.3)
```

### Fitting to an experiment's data
We now fit a model to these data using maximum likelihood estimation. To do so, we use the `log_probability` function. We first show how the log-likelihood varies as $a$ is varied with $h$ fixed at its true value. The log-likelihood is a little rough (owing to the stochasticity in how we calculate it), but it is peaked near the value used to generate the data.
```{r}
as <- seq(1, 4, 0.1)
log_likelihood <- vector(length = length(as))

for(i in seq_along(as)) {
  parameters <- list(a = as[i], h = true_parameters$h)
  log_likelihood[i] <- log_probability(
    parameters = parameters,
    data = df,
    model = model_rogersII(),
    n_replicates = 10000)
}

# plot
tibble(a=as, log_likelihood=log_likelihood) %>% 
  ggplot(aes(x=a, y=log_likelihood)) +
  geom_line() +
  geom_vline(xintercept = true_parameters$a,
             linetype=2,
             colour="orange")
```

Similarly so, for $h$.
```{r}
hs <- seq(0.01, 0.25, 0.01)
log_likelihood <- vector(length = length(hs))

for(i in seq_along(hs)) {
  parameters <- list(a = true_parameters$a, h=hs[i])
  log_likelihood[i] <- log_probability(
    parameters = parameters,
    data = df,
    model = model_rogersII(),
    n_replicates = 1000)
}

# plot
tibble(h=hs, log_likelihood=log_likelihood) %>% 
  ggplot(aes(x=h, y=log_likelihood)) +
  geom_line() +
  geom_vline(xintercept = true_parameters$h, linetype=2)
```

We can also look at the 2D likelihood surface. Here, we mark the true parameter set as a point.
```{r}
parameter_combinations <- expand_grid(a=as, h=hs)
parameter_combinations$z <- NA
for(i in seq_along(parameter_combinations$a)) {
  parameters <- list(a = parameter_combinations$a[i],
                     h = parameter_combinations$h[i])
  z <- log_probability(
    parameters = parameters,
    data = df,
    model = model_rogersII(),
    n_replicates = 1000) # smaller sample
  parameter_combinations$z[i] <- z
}

# plot
ggplot(parameter_combinations, aes(x = a, y = h, fill = z)) +
  geom_raster() +
  scale_fill_viridis_c() +  # Using continuous viridis color scale
  theme_minimal() +
  geom_point(data=data.frame(h=true_parameters$h, a=true_parameters$a)) +
  geom_contour(aes(z = z), color = "white")
```


We now perform optimisation to estimate a single maximum likelihood set of $(a,h)$.
```{r}
# set a function to minimise
f_likelihood <- function(theta) {
  parameters <- list(a=1.5, h=0.1) #  these are dummy values
  parameters[1] <- theta[1]
  parameters[2] <- theta[2]
  -log_probability( # minus sign needed as optim minimises
    parameters = parameters,
    data = df,
    model = model_rogersII(),
    n_replicates = 10000)
}

# use R's standard optim
fit <- optim(
  c(1.5, 0.1),
  f_likelihood,
  lower=c(0.01, 0.01),
  upper=c(5, 0.25)
)

# output pars
a <- fit$par[1]
h <- fit$par[2]
mle_parameters <- list(a=a, h=h)
print(paste0("a = ", a, ", h = ", h))
```

We then mark the MLE values as a cross on the 2D likelihood surface.
```{r}
# overlay on 2D plot
ggplot(parameter_combinations, aes(x = a, y = h, fill = z)) +
  geom_raster() +
  scale_fill_viridis_c() +  # Using continuous viridis color scale
  theme_minimal() +
  geom_point(data=data.frame(h=true_parameters$h, a=true_parameters$a)) +
  geom_point(data=data.frame(h=h, a=a), shape=4, size=4) +
  geom_contour(aes(z = z), color = "white")
```

### Model diagnostics
We now examine the fit of the model to data by generating data by simulation using the parameter estimates we obtained by optimisation. From this view, the fits look reasonable.
```{r}
experimental_setup <- data.frame(
  n_prey_initial = c(5, 10, 20, 30, 40),
  n_replicates = 1000
)

# generate synthetic data at max likelihood estimates
df_sim <- simulate_study(
  data=experimental_setup,
  time_max = 1,
  model = model_rogersII(),
  parameters = mle_parameters
)

# plot
df %>%
  ggplot(aes(x=as.factor(n_prey_initial), y=n_prey_eaten)) +
  geom_violin(data=df_sim) +
  geom_jitter(height = 0.3, alpha=0.8) +
  xlab("n_prey_initial")
```
We can also plot the empirical cumulative distribution function (eCDF) versus the model-simulated eCDFs. If the model fitted to the data is reasonable, we would expect these lines to overlap with the $y=x$ line.

Here, we plot these for each value of `n_prey_initial` across 100 bootstrapped samples. The model fit isn't perfect perhaps owing to the difficulty of maximising the likelihood here.
```{r}
# generate ecdfs
n_bootstraps <- 100
df_ecdfs_both <- create_bootstrapped_ecdf_real_simulated(
    n_bootstraps = n_bootstraps,
    data = df,
    time_max = 1,
    model = model_rogersII(),
    mle_parameters = mle_parameters
)

# plot
df_ecdfs_both %>% 
  ggplot(aes(x=ecdf_real, y=ecdf_sim, group=as.factor(bootstrap_id))) +
  geom_line(alpha=0.5) +
  facet_wrap(~n_prey_initial) +
  geom_abline(linetype=2, colour="orange")
```

As a sanity check, we repeat these checks using the true parameter values. This shows a slightly more reasonable fit.
```{r}
# generate ecdfs
n_bootstraps <- 100
df_ecdfs_both <- create_bootstrapped_ecdf_real_simulated(
    n_bootstraps = n_bootstraps,
    data = df,
    time_max = 1,
    model = model_rogersII(),
    mle_parameters = true_parameters
)

# plot
df_ecdfs_both %>% 
  ggplot(aes(x=ecdf_real, y=ecdf_sim, group=as.factor(bootstrap_id))) +
  geom_line(alpha=0.5) +
  facet_wrap(~n_prey_initial) +
  geom_abline(linetype=2, colour="orange")
```

